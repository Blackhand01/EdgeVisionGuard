(8) I 20 migliori progetti di computer vision per il 2025! - YouTube
https://www.youtube.com/watch?v=eCDTp5h59hg

Transcript:
(00:00) hi everyone welcome back to our channel in this video we will discuss some great computer vision projects without further Ado let's start with number 20 building a robotic arm that can pick up and sort objects is no easy task but Kai and his friends took on the challenge of developing their own automated system their goal to detect colored balls and sort them accurately using a budget friendly robotic manipulator they use the open manipulator x a lowcost robotic arm with four joints giving it 4Â° of movement a
(00:34) standard USB camera served as the robot's eye identifying the balls based on their color to ensure precise movement they first calculated the arms position using forward and inverse kinematics to sort the balls they corrected the camera's Distortion and processed the image to detect colors and positions once everything synced the robot seamlessly scanned grabbed and sorted the balls number 19 imagine walking past a booth at a maker fair and a robot suddenly locks eyes with you tracking your every move that's exactly
(01:09) what this Creator built a custom animatronic inspired by love death and robots featuring realistic eye movement and face detection the build started with a sculpted model cast in silicone molds with resin used for the final shell inside an Nvidia Jetson runs face detection allowing the robot's camera to track faces and adjust its gaze accordingly the eye mechanism is powered by eight servos individually controlling eye movement and blinking for added realism a Raspberry Pi 4 handles additional interactions using lidar to
(01:45) trigger sounds and lights when visitors get close while still a prototype the Creator plans to refine the tracking system to reduce overshooting and improve responsiveness if you ever see this robot don't be surprised if it stares right back at you number 18 koshiro has built a ball balancing robot that keeps a ball from ever falling off a glass plate but how does it work koshiro has explained the concept in detail but here's a simpler way to understand it beneath the plate there's a camera that captures the ball's position and image
(02:20) processing is used to track its coordinates an algorithm then checks how far the ball has deviated from the center and instructs the motors to move the ball back to that position this way the ball always stays on the platform number 17 let's look how the coders Cafe team is innovating to assist individuals with speech impairments in communicating more effectively they've developed a wearable device in the form of spectacles that can detect hand symbols and translate them into audible speech in real time the spectacle frame is 3D printed
(02:55) from pla filament while the design may not prioritize ergonomics it does have an appealing aesthetic a small pie camera is embedded at the center of the frame capturing hand gestures these gestures are then processed through a machine learning model which identifies the symbols and converts them into spoken words via a speaker this project is truly amazing offering a practical solution to a real life problem number 16 V projects has created a tiny device that alerts you when your boss is checking on you while
(03:28) you're busy playing games on your laptop it's like having an extra set of eyes watching your back the device uses a Raspberry Pi server and a detection kit the server is built with a pi 5 a Raspberry Pi AI kit and a camera module while the detection kit includes a pi pico2 a voltmeter a mosfet an LED and some resistors when the server detects someone in the frame it sends an alert to the detection kit the LED on the detector lights up and the dial moves towards one indicating a higher likelihood that someone is nearby number
(04:04) 15 this robot can detect and put out fires all on its own instead of relying on traditional fire sensors it uses image recognition with the Grove Vision AI module to spot flames in real time built with an Arduino Nano a Raspberry Pi camera module and a fan for extinguishing fires it runs on a trained AI model designed for accuracy to ensure the best performance two machine Lear learning models were tested one with sense craft Ai and another with Edge impulse after comparison The Edge impulse model proved to be the better
(04:40) choice the model was optimized to ensure it could run smoothly on the embedded system once deployed the robot can detect fire and activate the fan to put it out making it a useful safety tool number 14 recognizing vehicle number plates automatically can be useful for smart parking toll booths and security systems the hardware setup includes an ESP 32 an OLED display and a trigger button pressing the button captures an image and sends it to the Cloud Server via Wi-Fi the server processes the image extracts the license plate number and
(05:16) Returns the result to the esp32 which displays it on the screen during testing the system successfully identified multiple license plates but accuracy varied depending on lighting and font clarity while reliable in most cases certain characters like o and Q could be misread despite these minor limitations this project demonstrates an affordable and efficient way to integrate automated license plate recognition into real world applications before moving on to the next project a word from our sponsor Altium 365 is a powerful PCB design
(05:52) platform that makes it easy to share your design files with your team members can view edit and comment in realtime stre streamlining collaboration need input from a software engineer they'll get notified and can take action right away with easy access to data sheets and materials you can even send your design directly to the manufacturer simplifying the process and boosting workflow efficiency if you're a student looking to Kickstart your PCB design career Altium student lab offers online courses and free access to their cuttingedge
(06:26) design software to help you master the basics of PCB design and ecad fundamentals check the description for more details number 13 if you've ever wanted to build your own AI powered camera this project is a great place to start Eric designed a simple yet effective setup using an esp32 cam inside a cardboard box along with a push button to capture images and a screen to display them but how does it recognize objects here's how it works the ESP 32 cam captures an image and uploads it to a nodejs server which then calls the
(07:04) Google Vision API for image analysis once the object is identified the processed image is sent back to the ESP 32 cam providing detailed information about what it sees with this setup you can easily identify and analyze objects in real time number 12 meet auto bill a fast and effective setup to generate shopping bills thus reducing the checkout time this project might look like a product but it is completely DIY it identifies the objects using a camera placed above while the load cell at the bottom weighs the
(07:43) object the object along with its weight price and quantity is then automatically added to the cart and the bill is generated thus omitting the need for human involvement the users can simply pay the total amount and collect their groceries thus it reduces es the chances of forming long cues number 11 tired of standing over a trash bin second guessing where to toss that plastic wrapper the makers at Microsoft built a Raspberry Pi powered trash classifier that instantly tells you where an item belongs Landfill Recycling compost or
(08:20) hazardous waste using lobe a no code machine learning tool they trained a tensorflow model by feeding it images of different types of waste the model runs on a Raspberry Pi 4 which is housed in a custombuilt case with LED indicators and a push button to snap a photo once an item is scanned the system analyzes it and lights up the correct disposal bin no more guessing this project could make waste sorting way easier for everyone so would you build one number 10 manually sorting objects can be a tedious task but this project makes it effortless
(08:57) with automation here's how it works a Servo powered gripper picks up the objects and places them onto a conveyor belt as the belt moves an IR sensor detects an object and pauses the belt when it reaches the camera the captured image is then sent to a web browser where tensorflow light performs image classification once the object is identified the conveyor resumes movement and sorts the item accordingly making the entire process efficient and hands-free number nine with this project you can detect hands and fingers using Python
(09:37) and transform them into a gesture controlled virtual Mouse in this demo a webcam tracks hand movements demonstrating how image recognition works in real time moving your index finger controls the cursor in any direction while lowering your middle finger with the index finger still up acts as a click making interactions seamless and in itive to run this on an ESP 32 cam you only need to adjust the dimensions and frame rate in the code everything else Remains the Same ensuring smooth integration number eight running open CV
(10:14) on an ESP 32 cam is not that easy but by following this project you can do it esp32 cam captures the image and uses canny Edge detection with the open CV library in real time once the code runs the screen displays the ttgo demo for 3 seconds after that RGB mode grayscale mode binarized mode and Edge mode are executed repeatedly The canny Edge detection requires the most computational power hence the FPS of the displayed image is low but on average it displays the image with 6 FPS number seven with this project you'll learn how
(10:55) to build a QR code scanner using the esp32 cam module and open CV QR codes come in different versions based on the amount of data they store and this project allows you to decode even complex ones with ease to scan a QR code a python library is used to process the image frames captured by the es32 cam the code extracts these frames deciphers the QR code using a library function and then displays the decoded data on the screen in real time number six can a football game and become a high-tech interactive experience without breaking
(11:33) the bank mortaza set out to build a tic-tac-toe football wall not with expensive sensors but using computer vision and webcams traditional interactive football setups cost upwards of $133,000 due to specialized wall sensors instead this project uses two cameras a projector and AI detection to track ball hits One camera detects when the ball makes contact while while the second determines where it lands with yolo object detection a custom Tred model improves accuracy identifying ball hits in real time ensuring accurate ball
(12:11) tracking required fine-tuning the detection model and adjusting the cooldown time to prevent multiple detections per hit after 3 days of testing the system works with 70 to 90% accuracy making it perfect for casual play number five improper waste disposal is one of the concerning ways that is polluting the environment to address this issue octar decided to build a neural network-based robot that can classify different kinds of trash with the help of edge impulse it runs on Python scripts and can be operated remotely the laser scanner
(12:46) guides the robot around its environment thus avoiding obstacles as it travels around it captures images with the webcam which are classified by the neural network ensuring efficient sorting and Disposal number four counting some limited objects is easy but if the number is too large then you may definitely need some help for that this little project can be of great use to detect the object it uses a step-by-step method of image or frame conversion initially the RGB image is converted to grayscale so that the
(13:21) color contrast is visible and mathematical operations can be performed easily after that the image is blurred to blend the colors and using canny Edge detection the edges are detected finally to properly join the detected edges the image is dilated in this way the number of closed figures is detected and printed on the serial monitor number three what if you could play Minecraft without a keyboard or Mouse Gabriel built an AI power gesture control system that lets you move mine and interact all with hand movements the
(13:56) program runs on Google's media pipe AI which tracks hand landmarks and calculates distances between key points open CV helps estimate head size for scaling movements while a custom algorithm translates gestures into in-game actions instead of relying on traditional input methods the system uses multi-threading to process each action separately keeping gameplay smooth but there's a catch it's not perfect running requires rapid hand spasms crafting is a struggle and combat is really difficult still for a project
(14:29) that imagines how we interact with games it's an impressive feat number two you've probably seen this Eerie TV effect in horror movies the one where a ghost appears beside you on the screen but the moment you turn to look it vanishes spooky right but how does it actually work this illusion is brought to life using an old black and white TV hooked up to a Raspberry Pi 3 the trick lies in a small camera hidden inside the screen's headphone jack which captures the scene in front of the TV the footage is then processed
(15:02) through open CV running face and eye detection algorithms to track when someone is looking at the screen when the viewer looks away from the screen a ghostly figure suddenly appears lurking behind them but the moment they turn to check the Apparition vanishes just like in classic horror movie tropes it's a simple yet incredibly effective way to Spook anyone number one in this project you'll learn how to run depth AI on a Raspberry Pi using an oakd light camera enabling real-time object detection and spatial awareness
(15:38) this setup allows you to gather detailed information about an object's position in physical space to get started you'll need to install depth AI on the Raspberry Pi once the demo is running two screens will be displayed the left screen shows a color feed captured by the center lens providing a standard visual reference while the right screen presents a stereo depth View using both side lenses this depth screen displays an extended disparity map which helps calculate distances and create a 3D representation of the
(16:10) surroundings with this you can accurately detect objects and their positions in real time making it ideal for AI Vision applications if you've made it till here then drop a like And subscribe to our channel to keep supporting us comment the project that you loved the most we will be back with some great ideas soon till then goodbye
